# -*- coding: utf-8 -*-
"""colab_inference_server.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GJCwGVeqtXZXeail-K_wmBvNB8MfG6QC
"""

!pip install pyngrok
#!pip install googletrans==4.0.0-rc1
!pip install fastapi uvicorn nest_asyncio pyngrok transformers accelerate torch --quiet

from pyngrok import ngrok
from huggingface_hub import login

#ngrok 토큰
ngrok.set_auth_token("")
#huggingface 토큰
login("")

# from googletrans import Translator

# #번역 테스트
# translator = Translator()
# text = "후쿠오카에서 가볼만한 명소는 어디야?"
# translated = translator.translate(text, src="ko", dest="en")
# print(translated.text)

from fastapi import FastAPI, Request
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM
from pyngrok import ngrok
import nest_asyncio
import uvicorn
import torch


# 모델 불러오기
# tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")
# model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")

# tokenizer = AutoTokenizer.from_pretrained("maywell/KULLM-polyglot-5.8B-v2")
# model = AutoModelForCausalLM.from_pretrained("maywell/KULLM-polyglot-5.8B-v2", device_map="auto", torch_dtype="auto")

# tokenizer = AutoTokenizer.from_pretrained("skt/kogpt2-base-v2")
# model = AutoModelForCausalLM.from_pretrained("skt/kogpt2-base-v2")
# model.eval()

# model_name = "beomi/KoAlpaca-Polyglot-5.8B"
# model_name = "Bllossom/llama-3.2-Korean-Bllossom-3B"

# tokenizer = AutoTokenizer.from_pretrained(model_name)
# model = AutoModelForCausalLM.from_pretrained(
#     model_name,
#     device_map="auto",
#     torch_dtype="auto",
# )

model_name = "Bllossom/llama-3.2-Korean-Bllossom-AICA-5B"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)
model.eval()



# if torch.cuda.is_available():
#     model.to("cuda")

# 요청 형식 정의
# class QueryRequest(BaseModel):
#     prompt: str

# FastAPI 앱 초기화
app = FastAPI()

# POST 요청 처리
@app.post("/generate")
async def generate_answer(request: Request):
    body = await request.json()
    prompt = body.get("prompt", "")
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)

    with torch.no_grad():
        output = model.generate(
            input_ids=input_ids,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id  # 경고 방지용 설정
        )

    # 전체 디코딩 결과
    decoded = tokenizer.decode(output[0], skip_special_tokens=True)

    #  prompt 이후 생성된 부분만 추출
    if decoded.startswith(prompt):
        answer_text = decoded[len(prompt):].strip()
    else:
        answer_text = decoded

    return {"answer": answer_text}

@app.get("/")
async def root():
    return {"message": "FastAPI 서버가 실행 중입니다."}

# Colab 환경 대응 (ngrok + asyncio 우회)
ngrok.set_auth_token("2y2HkhTPMfheoJUMyrwPvB15fzd_41eeJhYEn5CCVWUeLuLth")
public_url = ngrok.connect(7860)
print("외부 접속 주소:", public_url)

# Colab 환경에서는 asyncio 충돌 우회 필요
nest_asyncio.apply()

# uvicorn 대신 내부에서 직접 서버 실행
uvicorn.run(app, host="0.0.0.0", port=7860)

